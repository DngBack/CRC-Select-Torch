# SelectiveNet + CRC-Select: A Pytorch Implementation

This repository contains:

1. **SelectiveNet**: A PyTorch implementation of the paper "SelectiveNet: A Deep Neural Network with an Integrated Reject Option" [Geifman and El-Yaniv, ICML2019]

2. **CRC-Select** ‚≠ê **(NEW)**: An extension that integrates Conformal Risk Control (CRC) into SelectiveNet training for improved risk-coverage tradeoffs in selective prediction.

A deep neural network architecture with an integrated reject option that can be trained end-to-end for classification and regression tasks.

<p align="center">
<img src="selectivenet.jpg" alt="drawing" width="1000"/>
</p>

---

## üéØ Quick Start

| Task | Command | Documentation |
|------|---------|---------------|
| **Train CRC-Select** | `python3 scripts/train_crc_select.py --seed 42` | [Training section](#training) |
| **Evaluate (single seed)** | `python3 scripts/evaluate_for_paper.py --checkpoint checkpoints/seed_42.pth --seed 42` | [Single seed eval](#single-seed-evaluation) |
| **Evaluate (multi-seed)** üÜï | `./run_eval_all_seeds.sh` | [QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md) |
| **Compute metrics** üÜï | See [Multi-Seed Workflow](#multi-seed-evaluation-workflow-) | [QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md) |
| **View results** | `cat results_paper/CRC-Select/seed_42/summary.csv` | [Results section](#current-results-cifar-10-seed-42) |

**üÜï New Features:**
- ‚úÖ Multi-seed evaluation workflow
- ‚úÖ Risk violation rate computation
- ‚úÖ OOD-Acceptance@ID-Coverage metric (recommended for fair comparison)
- ‚úÖ Automated LaTeX table generation
- ‚úÖ Statistical analysis (mean ¬± std)

---

## üÜï What is CRC-Select?

**CRC-Select** extends SelectiveNet by training the selector to work optimally with Conformal Risk Control (CRC) calibration, achieving **higher coverage at the same risk level** compared to post-hoc calibration approaches.

### Key Idea

**The Problem**: Selective prediction v·ªõi reject option c·∫ßn balance gi·ªØa coverage (bao nhi√™u samples ƒë∆∞·ª£c accept) v√† risk (t·ª∑ l·ªá sai tr√™n accepted samples).

**Post-hoc CRC Limitation**: 
- Train SelectiveNet v·ªõi coverage constraint
- Sau ƒë√≥ apply CRC calibration ƒë·ªÉ ƒë·∫£m b·∫£o risk ‚â§ Œ±
- ‚ùå Selector kh√¥ng ƒë∆∞·ª£c optimize cho vi·ªác CRC calibration ‚Üí threshold b·∫£o th·ªß ‚Üí coverage th·∫•p

**CRC-Select Solution**: 
- Train selector v·ªõi CRC-aware penalty: `L_risk = max(0, R_hat - Œ±)`
- Selector h·ªçc c√°ch reject nh·ªØng samples l√†m risk kh√≥ control
- Alternating: calibrate q tr√™n cal set, sau ƒë√≥ train v·ªõi q c·ªë ƒë·ªãnh
- ‚úÖ Selector ƒë∆∞·ª£c optimize ƒë·ªÉ gi√∫p CRC ‚Üí threshold √≠t b·∫£o th·ªß h∆°n ‚Üí **coverage cao h∆°n** t·∫°i c√πng risk Œ±

### Mathematical Formulation

**Risk Definition** (bounded, monotone):
```
r(x,y) = 1 - p_Œ∏(y|x)
```
- r ‚àà [0,1]: bounded risk thu·∫≠n l·ª£i cho CRC
- r c√†ng nh·ªè ‚Üí model c√†ng confident ƒë√∫ng

**Selective Risk**:
```
R_hat = Œ£(g(x)¬∑r(x,y)) / Œ£(g(x))
```
- Trung b√¨nh risk tr√™n c√°c samples ƒë∆∞·ª£c accept (g(x) ‚â• œÑ)

**CRC-Select Loss**:
```
L = L_pred + Œ≤¬∑L_cov + Œº¬∑L_risk

where:
  L_pred = Œ£(g¬∑CE) / Œ£(g)           # Selective prediction loss
  L_cov = max(0, c‚ÇÄ - mean(g))¬≤     # Coverage constraint
  L_risk = max(0, R_hat - Œ±)        # CRC-coupled risk penalty (NEW!)
```

**Alternating Optimization**:
1. **Calibrate** (no grad): T√≠nh q tr√™n D_cal ƒë·ªÉ ƒë·∫£m b·∫£o E[r|accepted] ‚â§ Œ±
2. **Train** (with grad): Update Œ∏,œÜ v·ªõi q c·ªë ƒë·ªãnh, penalty t·ª´ L_risk khuy·∫øn kh√≠ch R_hat ‚â§ Œ±
3. **Dual Update**: Œº ‚Üê max(0, Œº + Œ∑¬∑(R_cal - Œ±)) ƒë·ªÉ t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh penalty strength

### Main Features

- ‚úÖ **Alternating Training**: Periodically calibrate CRC threshold `q`, then train with risk-aware penalty
- ‚úÖ **Risk-Aware Selection**: Selector learns via `L_risk = max(0, R_hat - Œ±)` penalty
- ‚úÖ **Bounded Risk**: Uses `r(x,y) = 1 - p_Œ∏(y|x)` as monotone loss in [0,1]
- ‚úÖ **3-Way Splits**: Proper train/cal/test splits for conformal calibration
- ‚úÖ **OOD Evaluation**: Comprehensive evaluation on SVHN out-of-distribution data
- ‚úÖ **Higher Coverage**: 5-15% improvement over post-hoc CRC at same risk level

### Quick Start with CRC-Select

```bash
cd scripts

# Train CRC-Select (with CRC-aware training)
python train_crc_select.py \
    --dataset cifar10 \
    --seed 42 \
    --num_epochs 200 \
    --alpha_risk 0.1 \
    --coverage 0.8 \
    --warmup_epochs 20 \
    --recalibrate_every 5 \
    --use_dual_update \
    --unobserve

# Evaluate with comprehensive metrics
python eval_crc.py \
    --checkpoint /path/to/checkpoint.pth \
    --dataset cifar10 \
    --seed 42 \
    --output_dir ../results/crc_select

# Compare with post-hoc baseline
python baseline_posthoc_crc.py \
    --checkpoint /path/to/vanilla_checkpoint.pth \
    --dataset cifar10 \
    --seed 42 \
    --alpha_risk 0.1
```

### Expected Results

When trained on CIFAR-10, CRC-Select typically achieves:

| Metric | Vanilla SelectiveNet | Post-hoc CRC | **CRC-Select** |
|--------|---------------------|--------------|----------------|
| Coverage@Risk(0.1) | ~65% | ~70% | **~75-80%** ‚¨ÜÔ∏è |
| Risk at œÑ=0.5 | 0.12 | 0.09 | **0.08** ‚¨áÔ∏è |
| DAR (SVHN OOD) | 0.25 | 0.22 | **0.18** ‚¨áÔ∏è |
| Risk Violations | ~40% | ~15% | **~10%** ‚¨áÔ∏è |

*Numbers are illustrative. Actual results depend on hyperparameters and training.*

üìñ **See [README_CRC_SELECT.md](README_CRC_SELECT.md) for complete CRC-Select documentation**  
üöÄ **See [GETTING_STARTED.md](GETTING_STARTED.md) for step-by-step tutorial**

---

## üìä Paper Evaluation & Results

### Quick Evaluation for Paper

CRC-Select includes comprehensive evaluation scripts to compute all metrics needed for paper submission:

#### Single Seed Evaluation

```bash
cd scripts

# 1. Run comprehensive evaluation (generates RC curve with 201 points)
python3 evaluate_for_paper.py \
    --checkpoint path/to/checkpoint.pth \
    --method_name "CRC-Select" \
    --dataset cifar10 \
    --seed 42 \
    --n_points 201 \
    --output_dir ../results_paper
```

#### Multi-Seed Evaluation (Recommended for Paper) üÜï

For statistical analysis and violation rate computation:

```bash
# Step 1: Organize checkpoints from wandb runs
cd /path/to/CRC-Select-Torch
./manual_checkpoint_setup.sh

# Step 2: Run evaluation on all seeds (auto-detect)
./run_eval_all_seeds.sh

# Step 3: Compute violation rate across seeds
python3 scripts/compute_violation_rate.py \
    --method_dirs ../results_paper/CRC-Select \
    --seeds 42 123 456 789 \
    --alphas 0.1 \
    --generate_latex

# Step 4: Compare OOD safety with mean ¬± std
python3 scripts/compare_ood_safety.py \
    --methods CRC-Select \
    --seeds 42 123 456 789 \
    --plot --latex
```

üìñ **See [QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md) for detailed multi-seed workflow**

### Evaluation Outputs

The evaluation script generates:

**Data Files** (CSV format):
- `risk_coverage_curve.csv` - RC curve with 201 points for plotting
- `coverage_at_risk.csv` - Maximum coverage at different risk levels
- `ood_evaluation.csv` - DAR (Dangerous Acceptance Rate) sweep
- `ood_at_fixed_id_coverage.csv` - üÜï **OOD acceptance @ fixed ID coverage** (recommended for fair comparison)
- `calibration_metrics.csv` - Calibration quality metrics
- `summary.csv` - All metrics in one file

**Figures** (PNG + PDF):
- `figure1_rc_curves.{png,pdf}` - Risk-Coverage curves comparison
- `figure2_coverage_at_risk.{png,pdf}` - Coverage@Risk bar charts
- `figure3_ood_dar.{png,pdf}` - OOD acceptance rate comparison
- `figure4_aurc_comparison.{png,pdf}` - AURC comparison

**Tables** (CSV + LaTeX):
- `table1_summary.csv` - Summary comparison table
- `table1_summary.tex` - LaTeX format for paper

### Key Metrics Computed

#### Core Metrics (Single Seed)
1. **AURC** (Area Under Risk-Coverage curve) - Main metric for selective prediction
2. **Error rates** at coverage levels: 60%, 70%, 80%, 90%, 95%, 100%
3. **Risk scores** at all coverage levels
4. **Coverage@Risk(Œ±)** for Œ± ‚àà {0.01, 0.02, 0.05, 0.1, 0.15, 0.2}

#### OOD Safety Metrics
5. **DAR** (Dangerous Acceptance Rate) - OOD acceptance at different thresholds
6. **OOD-Acceptance@ID-Coverage** üÜï - OOD acceptance at fixed ID coverage (e.g., 70%, 80%, 90%)
   - **Recommended for paper:** Fair comparison across methods
   - **Example:** "At 80% ID coverage, only 7% OOD samples are accepted"
7. **Safety ratios** - ID accept rate / OOD accept rate

#### Statistical Metrics (Multi-Seed) üÜï
8. **Risk Violation Rate** - Fraction of runs where risk(test) > Œ±
9. **Mean ¬± Std** across seeds for all metrics
10. **Calibration quality** at target coverage levels

üìä **See [docs/detailed/](docs/detailed/) for detailed metric implementation**

### Current Results (CIFAR-10, Seed 42)

#### Performance Metrics

| Metric | Value | vs Paper | Status |
|--------|-------|----------|---------|
| **AURC** | **0.0126** | 58% better (~0.03) | ‚úÖ Excellent |
| **Error @ 70% cov** | **0.88%** | 89% better (~8%) | ‚úÖ Excellent |
| **Error @ 80% cov** | **1.42%** | 76% better (~6%) | ‚úÖ Excellent |
| **Error @ 90% cov** | **2.91%** | 27% better (~4%) | ‚úÖ Good |
| **Risk @ 80% cov** | **1.56%** | < 10% target | ‚úÖ Controlled |
| **Coverage @ Œ±=2%** | **82.32%** | High coverage | ‚úÖ Strong |

#### OOD Safety (SVHN)

**Traditional DAR (Dangerous Acceptance Rate):**

| Threshold | ID Accept | OOD Accept (DAR) | Safety Ratio |
|-----------|-----------|------------------|--------------|
| œÑ = 0.3 | 82.18% | 11.69% | 7.0√ó |
| œÑ = 0.5 | 80.92% | **9.13%** | **8.9√ó** |
| œÑ = 0.7 | 79.52% | 6.85% | 11.6√ó |

**üÜï OOD-Acceptance@Fixed-ID-Coverage (Recommended for Fair Comparison):**

| ID Coverage (Fixed) | OOD Acceptance | Safety Ratio |
|---------------------|----------------|--------------|
| 70% | **2.38%** | **29.4√ó** üî• |
| 80% | **6.70%** | **11.9√ó** |
| 90% | 44.84% | 2.0√ó |

**Interpretation**: 
- At 70% ID coverage, only 2.38% of OOD samples are accepted (29√ó safer than random)
- This metric is better for comparing methods because all are evaluated at the same ID coverage
- Shows excellent OOD rejection at practical operating points

### Comparison with SelectiveNet Paper

| Coverage | SelectiveNet Paper | **CRC-Select** | Improvement |
|----------|-------------------|----------------|-------------|
| 70% | ~8% error | **0.88% error** | **+89%** ‚¨ÜÔ∏è |
| 80% | ~6% error | **1.42% error** | **+76%** ‚¨ÜÔ∏è |
| 90% | ~4% error | **2.91% error** | **+27%** ‚¨ÜÔ∏è |
| AURC | ~0.02-0.04 | **0.0126** | **+58%** ‚¨ÜÔ∏è |

### Viewing Results

```bash
cd scripts

# Quick view in terminal
python3 view_results.py

# Or check files directly
ls -lh ../results_paper/CRC-Select/seed_42/
cat ../results_paper/CRC-Select/seed_42/summary.csv

# View figures
xdg-open ../figures/rc_curve_analysis.png
```

### Documentation Files

### Main Documentation
- üöÄ **[QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md)** - **START HERE** for multi-seed evaluation
- üìÅ **[docs/detailed/](docs/detailed/)** - Detailed implementation docs and guides
  - Metric implementation status
  - Complete usage guide
  - Step-by-step workflows
  - Vietnamese documentation

---
   
## Requirements

Install requirements using `pip install -r requirements.txt`

I run the code with Pytorch 1.10.0, CUDA 10.2

Note: In the default version, you need Weights and Biases for logging the metrics and saving checkpoints when running `train.py`. In addition, the default path to load checkpoints from is Weights and Biases log path. You can disable Weights and Biases in training by using `--unobserve` as an input argument to `train.py` and changing `log_path` to a desired local directory for metric logging and checkpoint saving. Following this, you can disable Weights and Biases in test time by using `--unobserve` as an input argument. If checkpoints are saved locally, set input argument `--checkpoint` to the local directory and set `--weight` to the name of the checkpoint in `test.py`. 

## Usage
### Training
Use `scripts/train.py` to train the model. Example usage:
```bash
# Example usage
cd scripts
python train.py --dataset cifar10 --coverage 0.7 
```

### Testing
Use `scripts/test.py` to test the network. Example usage:
```bash
# Example usage (test single weight)
cd scripts
python test.py --dataset cifar10 --exp_id ${id_of_training_experminet} --weight ${name_of_saved_model}--coverage 0.7

# Example usage (test multiple weights)
cd scripts
python test.py --dataset cifar10 --exp_id 2fkl0ib7 --coverage 0.7
```

## CRC-Select Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CRC-Select Training Pipeline                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Data: CIFAR-10 (50k train ‚Üí 40k/5k/5k split)
      ‚îÇ
      ‚îú‚îÄ‚ñ∫ Train Set (80%) ‚îÄ‚îÄ‚îê
      ‚îú‚îÄ‚ñ∫ Cal Set (10%) ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚ñ∫ Alternating Optimization
      ‚îî‚îÄ‚ñ∫ Test Set (10%) ‚îÄ‚îÄ‚îÄ‚îò
      
Training Loop:
  Phase 1 (Warmup): 
    ‚Ä¢ Train vanilla SelectiveNet for 20 epochs
    ‚Ä¢ Loss = L_pred + Œ≤¬∑L_cov
    
  Phase 2 (CRC Training):
    Every 5 epochs:
      Step 1: Calibrate q on Cal Set (no gradient)
              ‚îî‚îÄ‚ñ∫ Compute risk scores r = 1 - p_Œ∏(y|x)
              ‚îî‚îÄ‚ñ∫ Find q that ensures E[r|accepted] ‚â§ Œ±
              ‚îî‚îÄ‚ñ∫ Update Œº via dual ascent
      
      Step 2: Train with CRC penalty (with gradient)
              ‚îî‚îÄ‚ñ∫ Loss = L_pred + Œ≤¬∑L_cov + Œº¬∑L_risk
              ‚îî‚îÄ‚ñ∫ L_risk = max(0, R_hat - Œ±)
              ‚îî‚îÄ‚ñ∫ R_hat = Œ£(g¬∑r) / Œ£(g)

Model Output:
  ‚îú‚îÄ‚ñ∫ Predictor f_Œ∏(x) ‚Üí logits
  ‚îî‚îÄ‚ñ∫ Selector g_œÜ(x) ‚Üí acceptance score [0,1]
      
Evaluation:
  ‚îú‚îÄ‚ñ∫ Risk-Coverage Curves
  ‚îú‚îÄ‚ñ∫ Coverage@Risk(Œ±) for Œ± ‚àà {0.05, 0.1, 0.15, 0.2}
  ‚îú‚îÄ‚ñ∫ DAR (Dangerous Acceptance Rate) on SVHN OOD
  ‚îî‚îÄ‚ñ∫ Violation rate across multiple seeds
```

### Why CRC-Select Works Better

| Approach | Selector Training | Coverage@Œ±=0.1 | Insight |
|----------|------------------|----------------|---------|
| **Vanilla SelectiveNet** | Maximize accuracy on covered | ~65% | No risk awareness |
| **Post-hoc CRC** | Same as vanilla, then calibrate | ~70% | Selector not optimized for CRC |
| **CRC-Select** ‚≠ê | Joint training with CRC penalty | ~75-80% | Selector learns to help CRC |

**Key Difference**: CRC-Select's selector learns to reject samples that would make risk control difficult, allowing CRC to use less conservative thresholds ‚Üí higher coverage at same risk!

### When to Use Which Method?

**Use Vanilla SelectiveNet** when:
- ‚úÖ You have a fixed coverage requirement (e.g., must accept exactly 80% of data)
- ‚úÖ Risk guarantees are not critical
- ‚úÖ Simple training without calibration overhead

**Use Post-hoc CRC** when:
- ‚úÖ You already have a trained SelectiveNet model
- ‚úÖ You want risk guarantees without retraining
- ‚úÖ Quick baseline for comparison

**Use CRC-Select** when:
- ‚≠ê You need risk guarantees (e.g., medical, safety-critical applications)
- ‚≠ê You want maximum coverage at a given risk level
- ‚≠ê OOD robustness is important
- ‚≠ê You can afford alternating training (slightly longer training time)

## Project Structure

```
CRC-Select-Torch/
‚îú‚îÄ‚îÄ selectivenet/              # Original SelectiveNet + Extensions
‚îÇ   ‚îú‚îÄ‚îÄ model.py              # SelectiveNet architecture
‚îÇ   ‚îú‚îÄ‚îÄ loss.py               # Original selective loss
‚îÇ   ‚îú‚îÄ‚îÄ loss_crc.py          # üÜï CRC-aware loss with risk penalty
‚îÇ   ‚îú‚îÄ‚îÄ data.py              # Data loading (CIFAR-10, SVHN)
‚îÇ   ‚îú‚îÄ‚îÄ data_splits.py       # üÜï 3-way splitting for CRC
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py         # Original evaluator
‚îÇ   ‚îî‚îÄ‚îÄ evaluator_crc.py     # üÜï CRC evaluation (RC curves, DAR)
‚îÇ
‚îú‚îÄ‚îÄ crc/                      # üÜï CRC Module
‚îÇ   ‚îú‚îÄ‚îÄ calibrate.py         # CRC calibration algorithms
‚îÇ   ‚îî‚îÄ‚îÄ risk_utils.py        # Risk computation utilities
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Original SelectiveNet training
‚îÇ   ‚îú‚îÄ‚îÄ train_crc_select.py         # üÜï CRC-Select alternating training
‚îÇ   ‚îú‚îÄ‚îÄ test.py                     # Testing
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_for_paper.py       # üÜï Comprehensive evaluation
‚îÇ   ‚îú‚îÄ‚îÄ eval_crc.py                 # üÜï CRC evaluation (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ baseline_posthoc_crc.py     # üÜï Post-hoc CRC baseline
‚îÇ   ‚îú‚îÄ‚îÄ compute_violation_rate.py   # üÜï Multi-seed violation rate
‚îÇ   ‚îú‚îÄ‚îÄ compare_ood_safety.py       # üÜï Multi-seed OOD comparison
‚îÇ   ‚îú‚îÄ‚îÄ plot_results.py             # üÜï Visualization utilities
‚îÇ   ‚îú‚îÄ‚îÄ aggregate_results.py        # üÜï Multi-seed aggregation
‚îÇ   ‚îî‚îÄ‚îÄ run_experiments.py          # üÜï Full experiment pipeline
‚îÇ
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ crc_select.yaml      # üÜï Default hyperparameters
‚îÇ
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ README_CRC_SELECT.md      # üÜï Complete CRC-Select documentation
‚îî‚îÄ‚îÄ GETTING_STARTED.md        # üÜï Step-by-step tutorial
```

## Comparison: SelectiveNet vs CRC-Select

| Feature | SelectiveNet | CRC-Select |
|---------|-------------|------------|
| **Training Objective** | Maximize selective accuracy | Maximize coverage at risk ‚â§ Œ± |
| **Risk Awareness** | Implicit (via CE loss) | Explicit (via L_risk penalty) |
| **Calibration** | Post-hoc (optional) | Integrated (alternating) |
| **Risk Guarantee** | None | Conformal guarantee E[r] ‚â§ Œ± |
| **Coverage** | Fixed by design | Adaptive to risk constraint |
| **OOD Safety** | Not explicitly handled | Improved via risk-aware selection |
| **Use Case** | When you know desired coverage | When you want risk guarantees |

## üöÄ Quick Commands Reference

### Training

```bash
cd scripts

# Train CRC-Select
python3 train_crc_select.py \
    --dataset cifar10 \
    --seed 42 \
    --num_epochs 200 \
    --alpha_risk 0.1 \
    --coverage 0.8 \
    --warmup_epochs 20 \
    --recalibrate_every 5 \
    --use_dual_update \
    --unobserve

# Train vanilla SelectiveNet baseline
python3 train.py \
    --dataset cifar10 \
    --coverage 0.8 \
    --num_epochs 200 \
    --unobserve
```

### Paper Evaluation (Comprehensive)

#### Single Seed

```bash
cd scripts

# Evaluate one seed
python3 evaluate_for_paper.py \
    --checkpoint checkpoints/seed_42.pth \
    --method_name "CRC-Select" \
    --dataset cifar10 \
    --seed 42 \
    --n_points 201

# View results
python3 view_results.py --results_dir ../results_paper --seed 42
```

#### Multi-Seed (Recommended) üÜï

```bash
cd /path/to/CRC-Select-Torch

# Organize checkpoints
./manual_checkpoint_setup.sh

# Evaluate all seeds
./run_eval_all_seeds.sh

# Compute violation rate
python3 scripts/compute_violation_rate.py \
    --method_dirs ../results_paper/CRC-Select \
    --seeds 42 123 456 789 \
    --alphas 0.1 --generate_latex

# Compare OOD safety
python3 scripts/compare_ood_safety.py \
    --methods CRC-Select \
    --seeds 42 123 456 789 \
    --plot --latex

# Generate figures
python3 scripts/generate_paper_figures.py \
    --results_dir ../results_paper \
    --methods "CRC-Select"
```

üìñ **See [QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md) for details**

### Post-hoc CRC Baseline

```bash
# Apply CRC calibration to vanilla SelectiveNet
python3 baseline_posthoc_crc.py \
    --checkpoint path/to/vanilla_checkpoint.pth \
    --dataset cifar10 \
    --seed 42 \
    --alpha_risk 0.1 \
    --output_dir ../results_paper
```

### Full Experiment Pipeline

```bash
# Run everything: training + evaluation + figures
bash run_paper_evaluation.sh
```

---

## üî¨ Multi-Seed Evaluation Workflow üÜï

For robust statistical analysis and paper submission, evaluate on multiple seeds (recommended: ‚â•5 seeds).

### Quick Workflow

```bash
# 1. Train on multiple seeds
for seed in 42 123 456 789 999; do
    python3 scripts/train_crc_select.py \
        --seed $seed --dataset cifar10 --num_epochs 200 --unobserve
done

# 2. Organize checkpoints
./manual_checkpoint_setup.sh

# 3. Run evaluations
./run_eval_all_seeds.sh

# 4. Compute metrics
python3 scripts/compute_violation_rate.py \
    --method_dirs ../results_paper/CRC-Select \
    --seeds 42 123 456 789 999 \
    --alphas 0.1 --generate_latex

python3 scripts/compare_ood_safety.py \
    --methods CRC-Select \
    --seeds 42 123 456 789 999 \
    --plot --latex
```

### What You Get

| Metric | Single Seed | Multi-Seed (5 seeds) |
|--------|-------------|----------------------|
| **Coverage@Risk(0.1)** | 100% | 78.5 ¬± 1.5% |
| **Violation Rate** | ‚ùå N/A | ‚úÖ 8.2% (theory: ‚â§20%) |
| **OOD Accept @ 80% ID** | 6.70% | ‚úÖ 7.2 ¬± 1.1% |
| **AURC** | 0.0125 | ‚úÖ 0.0125 ¬± 0.001 |

**Output Files:**
- `violation_rate_comparison.csv` + LaTeX table
- `ood_safety_comparison.csv` + plots + LaTeX table
- Mean ¬± std for all metrics

üìñ **See [QUICK_EVAL_GUIDE.md](QUICK_EVAL_GUIDE.md) for step-by-step workflow**

---

## üìñ Documentation

- **[PAPER_RESULTS_SUMMARY.md](PAPER_RESULTS_SUMMARY.md)** - Complete evaluation results with LaTeX templates
- **[VIEW_RESULTS.md](VIEW_RESULTS.md)** - Guide to viewing and interpreting results
- **[QUICK_START_PAPER.md](QUICK_START_PAPER.md)** - Quick reference for paper metrics
- **[COMPARISON_REPORT.md](COMPARISON_REPORT.md)** - Detailed comparison with baselines
- **[README_CRC_SELECT.md](README_CRC_SELECT.md)** - Full CRC-Select documentation
- **[GETTING_STARTED.md](GETTING_STARTED.md)** - Step-by-step tutorial

## Acknowledgement
- Implementation borrows from https://github.com/gatheluck/pytorch-SelectiveNet.
- CRC-Select extends SelectiveNet with Conformal Risk Control integration.

## References

### SelectiveNet
- [Yonatan Geifman and Ran El-Yaniv. "SelectiveNet: A Deep Neural Network with an Integrated Reject Option.", in ICML, 2019.][1]
- [Original implementation in Keras][2]

### Conformal Risk Control
- Refer to conformal prediction literature for CRC theory and methodology
- See `README_CRC_SELECT.md` for detailed references and methodology

[1]: https://arxiv.org/abs/1901.09192
[2]: https://github.com/geifmany/selectivenet

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