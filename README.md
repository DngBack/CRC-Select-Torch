# SelectiveNet + CRC-Select: A Pytorch Implementation

This repository contains:

1. **SelectiveNet**: A PyTorch implementation of the paper "SelectiveNet: A Deep Neural Network with an Integrated Reject Option" [Geifman and El-Yaniv, ICML2019]

2. **CRC-Select** â­ **(NEW)**: An extension that integrates Conformal Risk Control (CRC) into SelectiveNet training for improved risk-coverage tradeoffs in selective prediction.

A deep neural network architecture with an integrated reject option that can be trained end-to-end for classification and regression tasks.

<p align="center">
<img src="selectivenet.jpg" alt="drawing" width="1000"/>
</p>

---

## ğŸ†• What is CRC-Select?

**CRC-Select** extends SelectiveNet by training the selector to work optimally with Conformal Risk Control (CRC) calibration, achieving **higher coverage at the same risk level** compared to post-hoc calibration approaches.

### Key Idea

**The Problem**: Selective prediction vá»›i reject option cáº§n balance giá»¯a coverage (bao nhiÃªu samples Ä‘Æ°á»£c accept) vÃ  risk (tá»· lá»‡ sai trÃªn accepted samples).

**Post-hoc CRC Limitation**: 
- Train SelectiveNet vá»›i coverage constraint
- Sau Ä‘Ã³ apply CRC calibration Ä‘á»ƒ Ä‘áº£m báº£o risk â‰¤ Î±
- âŒ Selector khÃ´ng Ä‘Æ°á»£c optimize cho viá»‡c CRC calibration â†’ threshold báº£o thá»§ â†’ coverage tháº¥p

**CRC-Select Solution**: 
- Train selector vá»›i CRC-aware penalty: `L_risk = max(0, R_hat - Î±)`
- Selector há»c cÃ¡ch reject nhá»¯ng samples lÃ m risk khÃ³ control
- Alternating: calibrate q trÃªn cal set, sau Ä‘Ã³ train vá»›i q cá»‘ Ä‘á»‹nh
- âœ… Selector Ä‘Æ°á»£c optimize Ä‘á»ƒ giÃºp CRC â†’ threshold Ã­t báº£o thá»§ hÆ¡n â†’ **coverage cao hÆ¡n** táº¡i cÃ¹ng risk Î±

### Mathematical Formulation

**Risk Definition** (bounded, monotone):
```
r(x,y) = 1 - p_Î¸(y|x)
```
- r âˆˆ [0,1]: bounded risk thuáº­n lá»£i cho CRC
- r cÃ ng nhá» â†’ model cÃ ng confident Ä‘Ãºng

**Selective Risk**:
```
R_hat = Î£(g(x)Â·r(x,y)) / Î£(g(x))
```
- Trung bÃ¬nh risk trÃªn cÃ¡c samples Ä‘Æ°á»£c accept (g(x) â‰¥ Ï„)

**CRC-Select Loss**:
```
L = L_pred + Î²Â·L_cov + Î¼Â·L_risk

where:
  L_pred = Î£(gÂ·CE) / Î£(g)           # Selective prediction loss
  L_cov = max(0, câ‚€ - mean(g))Â²     # Coverage constraint
  L_risk = max(0, R_hat - Î±)        # CRC-coupled risk penalty (NEW!)
```

**Alternating Optimization**:
1. **Calibrate** (no grad): TÃ­nh q trÃªn D_cal Ä‘á»ƒ Ä‘áº£m báº£o E[r|accepted] â‰¤ Î±
2. **Train** (with grad): Update Î¸,Ï† vá»›i q cá»‘ Ä‘á»‹nh, penalty tá»« L_risk khuyáº¿n khÃ­ch R_hat â‰¤ Î±
3. **Dual Update**: Î¼ â† max(0, Î¼ + Î·Â·(R_cal - Î±)) Ä‘á»ƒ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh penalty strength

### Main Features

- âœ… **Alternating Training**: Periodically calibrate CRC threshold `q`, then train with risk-aware penalty
- âœ… **Risk-Aware Selection**: Selector learns via `L_risk = max(0, R_hat - Î±)` penalty
- âœ… **Bounded Risk**: Uses `r(x,y) = 1 - p_Î¸(y|x)` as monotone loss in [0,1]
- âœ… **3-Way Splits**: Proper train/cal/test splits for conformal calibration
- âœ… **OOD Evaluation**: Comprehensive evaluation on SVHN out-of-distribution data
- âœ… **Higher Coverage**: 5-15% improvement over post-hoc CRC at same risk level

### Quick Start with CRC-Select

```bash
cd scripts

# Train CRC-Select (with CRC-aware training)
python train_crc_select.py \
    --dataset cifar10 \
    --seed 42 \
    --num_epochs 200 \
    --alpha_risk 0.1 \
    --coverage 0.8 \
    --warmup_epochs 20 \
    --recalibrate_every 5 \
    --use_dual_update \
    --unobserve

# Evaluate with comprehensive metrics
python eval_crc.py \
    --checkpoint /path/to/checkpoint.pth \
    --dataset cifar10 \
    --seed 42 \
    --output_dir ../results/crc_select

# Compare with post-hoc baseline
python baseline_posthoc_crc.py \
    --checkpoint /path/to/vanilla_checkpoint.pth \
    --dataset cifar10 \
    --seed 42 \
    --alpha_risk 0.1
```

### Expected Results

When trained on CIFAR-10, CRC-Select typically achieves:

| Metric | Vanilla SelectiveNet | Post-hoc CRC | **CRC-Select** |
|--------|---------------------|--------------|----------------|
| Coverage@Risk(0.1) | ~65% | ~70% | **~75-80%** â¬†ï¸ |
| Risk at Ï„=0.5 | 0.12 | 0.09 | **0.08** â¬‡ï¸ |
| DAR (SVHN OOD) | 0.25 | 0.22 | **0.18** â¬‡ï¸ |
| Risk Violations | ~40% | ~15% | **~10%** â¬‡ï¸ |

*Numbers are illustrative. Actual results depend on hyperparameters and training.*

ğŸ“– **See [README_CRC_SELECT.md](README_CRC_SELECT.md) for complete CRC-Select documentation**  
ğŸš€ **See [GETTING_STARTED.md](GETTING_STARTED.md) for step-by-step tutorial**

---
   
## Requirements

Install requirements using `pip install -r requirements.txt`

I run the code with Pytorch 1.10.0, CUDA 10.2

Note: In the default version, you need Weights and Biases for logging the metrics and saving checkpoints when running `train.py`. In addition, the default path to load checkpoints from is Weights and Biases log path. You can disable Weights and Biases in training by using `--unobserve` as an input argument to `train.py` and changing `log_path` to a desired local directory for metric logging and checkpoint saving. Following this, you can disable Weights and Biases in test time by using `--unobserve` as an input argument. If checkpoints are saved locally, set input argument `--checkpoint` to the local directory and set `--weight` to the name of the checkpoint in `test.py`. 

## Usage
### Training
Use `scripts/train.py` to train the model. Example usage:
```bash
# Example usage
cd scripts
python train.py --dataset cifar10 --coverage 0.7 
```

### Testing
Use `scripts/test.py` to test the network. Example usage:
```bash
# Example usage (test single weight)
cd scripts
python test.py --dataset cifar10 --exp_id ${id_of_training_experminet} --weight ${name_of_saved_model}--coverage 0.7

# Example usage (test multiple weights)
cd scripts
python test.py --dataset cifar10 --exp_id 2fkl0ib7 --coverage 0.7
```

## CRC-Select Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CRC-Select Training Pipeline                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Data: CIFAR-10 (50k train â†’ 40k/5k/5k split)
      â”‚
      â”œâ”€â–º Train Set (80%) â”€â”€â”
      â”œâ”€â–º Cal Set (10%) â”€â”€â”€â”€â”¤â”€â”€â–º Alternating Optimization
      â””â”€â–º Test Set (10%) â”€â”€â”€â”˜
      
Training Loop:
  Phase 1 (Warmup): 
    â€¢ Train vanilla SelectiveNet for 20 epochs
    â€¢ Loss = L_pred + Î²Â·L_cov
    
  Phase 2 (CRC Training):
    Every 5 epochs:
      Step 1: Calibrate q on Cal Set (no gradient)
              â””â”€â–º Compute risk scores r = 1 - p_Î¸(y|x)
              â””â”€â–º Find q that ensures E[r|accepted] â‰¤ Î±
              â””â”€â–º Update Î¼ via dual ascent
      
      Step 2: Train with CRC penalty (with gradient)
              â””â”€â–º Loss = L_pred + Î²Â·L_cov + Î¼Â·L_risk
              â””â”€â–º L_risk = max(0, R_hat - Î±)
              â””â”€â–º R_hat = Î£(gÂ·r) / Î£(g)

Model Output:
  â”œâ”€â–º Predictor f_Î¸(x) â†’ logits
  â””â”€â–º Selector g_Ï†(x) â†’ acceptance score [0,1]
      
Evaluation:
  â”œâ”€â–º Risk-Coverage Curves
  â”œâ”€â–º Coverage@Risk(Î±) for Î± âˆˆ {0.05, 0.1, 0.15, 0.2}
  â”œâ”€â–º DAR (Dangerous Acceptance Rate) on SVHN OOD
  â””â”€â–º Violation rate across multiple seeds
```

### Why CRC-Select Works Better

| Approach | Selector Training | Coverage@Î±=0.1 | Insight |
|----------|------------------|----------------|---------|
| **Vanilla SelectiveNet** | Maximize accuracy on covered | ~65% | No risk awareness |
| **Post-hoc CRC** | Same as vanilla, then calibrate | ~70% | Selector not optimized for CRC |
| **CRC-Select** â­ | Joint training with CRC penalty | ~75-80% | Selector learns to help CRC |

**Key Difference**: CRC-Select's selector learns to reject samples that would make risk control difficult, allowing CRC to use less conservative thresholds â†’ higher coverage at same risk!

### When to Use Which Method?

**Use Vanilla SelectiveNet** when:
- âœ… You have a fixed coverage requirement (e.g., must accept exactly 80% of data)
- âœ… Risk guarantees are not critical
- âœ… Simple training without calibration overhead

**Use Post-hoc CRC** when:
- âœ… You already have a trained SelectiveNet model
- âœ… You want risk guarantees without retraining
- âœ… Quick baseline for comparison

**Use CRC-Select** when:
- â­ You need risk guarantees (e.g., medical, safety-critical applications)
- â­ You want maximum coverage at a given risk level
- â­ OOD robustness is important
- â­ You can afford alternating training (slightly longer training time)

## Project Structure

```
CRC-Select-Torch/
â”œâ”€â”€ selectivenet/              # Original SelectiveNet + Extensions
â”‚   â”œâ”€â”€ model.py              # SelectiveNet architecture
â”‚   â”œâ”€â”€ loss.py               # Original selective loss
â”‚   â”œâ”€â”€ loss_crc.py          # ğŸ†• CRC-aware loss with risk penalty
â”‚   â”œâ”€â”€ data.py              # Data loading (CIFAR-10, SVHN)
â”‚   â”œâ”€â”€ data_splits.py       # ğŸ†• 3-way splitting for CRC
â”‚   â”œâ”€â”€ evaluator.py         # Original evaluator
â”‚   â””â”€â”€ evaluator_crc.py     # ğŸ†• CRC evaluation (RC curves, DAR)
â”‚
â”œâ”€â”€ crc/                      # ğŸ†• CRC Module
â”‚   â”œâ”€â”€ calibrate.py         # CRC calibration algorithms
â”‚   â””â”€â”€ risk_utils.py        # Risk computation utilities
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # Original SelectiveNet training
â”‚   â”œâ”€â”€ train_crc_select.py  # ğŸ†• CRC-Select alternating training
â”‚   â”œâ”€â”€ test.py              # Testing
â”‚   â”œâ”€â”€ eval_crc.py          # ğŸ†• Comprehensive CRC evaluation
â”‚   â”œâ”€â”€ baseline_posthoc_crc.py  # ğŸ†• Post-hoc CRC baseline
â”‚   â”œâ”€â”€ plot_results.py      # ğŸ†• Visualization utilities
â”‚   â”œâ”€â”€ aggregate_results.py # ğŸ†• Multi-seed aggregation
â”‚   â””â”€â”€ run_experiments.py   # ğŸ†• Full experiment pipeline
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ crc_select.yaml      # ğŸ†• Default hyperparameters
â”‚
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ README_CRC_SELECT.md      # ğŸ†• Complete CRC-Select documentation
â””â”€â”€ GETTING_STARTED.md        # ğŸ†• Step-by-step tutorial
```

## Comparison: SelectiveNet vs CRC-Select

| Feature | SelectiveNet | CRC-Select |
|---------|-------------|------------|
| **Training Objective** | Maximize selective accuracy | Maximize coverage at risk â‰¤ Î± |
| **Risk Awareness** | Implicit (via CE loss) | Explicit (via L_risk penalty) |
| **Calibration** | Post-hoc (optional) | Integrated (alternating) |
| **Risk Guarantee** | None | Conformal guarantee E[r] â‰¤ Î± |
| **Coverage** | Fixed by design | Adaptive to risk constraint |
| **OOD Safety** | Not explicitly handled | Improved via risk-aware selection |
| **Use Case** | When you know desired coverage | When you want risk guarantees |

## Acknowledgement
- Implementation borrows from https://github.com/gatheluck/pytorch-SelectiveNet.
- CRC-Select extends SelectiveNet with Conformal Risk Control integration.

## References

### SelectiveNet
- [Yonatan Geifman and Ran El-Yaniv. "SelectiveNet: A Deep Neural Network with an Integrated Reject Option.", in ICML, 2019.][1]
- [Original implementation in Keras][2]

### Conformal Risk Control
- Refer to conformal prediction literature for CRC theory and methodology
- See `README_CRC_SELECT.md` for detailed references and methodology

[1]: https://arxiv.org/abs/1901.09192
[2]: https://github.com/geifmany/selectivenet
